- Start Date: 2019-11-18
- RFC PR: 
- Tracking Issue: 

# Integrating Pachyderm with External Pipeline Systems via Pachyderm's S3 Gateway API

## Summary

We want to make it possible to integrate Pachyderm with other pipeline systems.
There are several elements to this, but the main one is enforcing the invariant
that pipelines only read from their input commits and write to their output commits,
so that Pachyderm's data lineage features continue to work. This will require
us to both make it natural and simple for external pipelines to read their
input commits, and unnatural and difficult to read from other commits.

In additional, we'll also need a way to trigger external pipelines when new
input commits arrive, but as detailed below, we don't foresee much complexity
here.

## Motivation

Currently, there are several external pipeline systems that users are either
already using or that don't naturally integrate with Pachyderm's model. For
example, Kubeflow makes it easy to run TensorFlow and PyTorch pipelines, which
pachyderm doesn't have any special integration with yet. Argo pipelines include
runtime conditionals and loops, which do not currently integrate well with
Pachyderm's data versioning model. Presto includes a SQL engine, which
Pachyderm does not.

However, none of these pipeline systems support data lineage, and all
of them are more or less mutually exclusive with each other. Our goals are to
support the features that these other systems support, and allowing Pachyderm
to trigger them means that Pachyderm could, with some operational work, be
a single system for all of these workloads.

Moreover, attaching a single input and output commit to each of these external
workloads allows complex runtime logic to integrate with Pachyderm's strong
data lineage model: external systems can implement any runtime logic at all,
and only the final product of that process will be versioned or transferred to
downstream pipelines.

## Detailed design

The triggering mechanism itself will initially be a Pachyderm pipeline that
calls out to an external system and monitors the external job to completion. On
exit, Pachyderm will gather all data generated by the external pipelines and
create a Pachyderm commit containing it.

The more interesting facet of this design is the storage interface. This hasn't been designed in detail, but we forsee the following issues:
- We don't want external pipelines communicating with pachd. Otherwise they may
  overwhelm pachd, which would both break the pipeline and make it impossible
  for the end user to kill it
- For now, we forsee configuring external pipelines to talk to the sidecar
  containers of the trigger pipeline. This will allow for parallel (and
  hopefully load-balanced) reads.
- We need to make it easy for external pipelines to read data from their input
  commit. The trigger pipeline could extract this information from its
  environment variables and pass it to the external pipeline, or the sidecar
  containers could manipulate incoming reads to only access the in-progress
  commit (this might preclude concurrent jobs from the same pipeline)
- We might need to make it possible for external pipelines to read their own
  writes to the output commit.

## How We Teach This

We don't forsee this feature introducing many new concepts. The main new
concept is that of an "external pipeline" and possibly, in addition, the
concept of a "trigger pipeline".

## Drawbacks

Tacitly encouraging users to run multiple pipelines systems (PPS, Argo, Spark,
etc) amounts imposing additonal operational complexity on them, and ultimately
on us as we try to support these users. If we instead ask users to wait until
PPS includes more of these features natively, even if that limits the number of
users of PPS in the short term, then we'll be able direct our very limited
support resources to a system that we already understand well and can control.

Similarly, trying to test our compatibility with every external pipeline system
that users might be interested in is likely to be difficult and expensive.
Neglecting this testing is likely to exacerbate our support burden.

## Alternatives

- Instead of S3 gateway, we could resume work on a Pachyderm CSI as the storage
  interface for these external pipelines

- Instead of a trigger pipeline, we could incorporate DSLs or special
  configuration syntax for each of these external pipeline types, and trigger
  them directly from within PPS.

## Unresolved questions

- Much of the design for the S3 gateway is TBD. I don't think any of us have
  enough experience running any of these other pipeline types to know what a
  good S3 interface would look like, or what's achievable.
